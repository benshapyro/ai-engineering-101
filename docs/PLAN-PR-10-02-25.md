# Implementation Plan for PR-10-02-25: Course Hardening & Productionization

**Document Version**: 1.0
**Created**: 2025-10-02
**Based On**: [PR-10-02-25.md](./PR-10-02-25.md)

---

## Executive Summary

**Scope**: 25 atomic commits transforming curriculum from tutorial to production-grade
**Estimated Effort**: 40-60 hours across 4-6 weeks
**Risk Level**: Medium-High (large refactoring with 848 TODOs to complete)
**Dependencies**: Requires OpenAI SDK update, new packages (sentence-transformers, pytest, FastAPI)

---

## Current State Analysis

### Existing Infrastructure âœ…

- âœ… 14 modules with examples, exercises, solutions, projects
- âœ… `shared/utils.py` with `LLMClient` class (uses chat.completions API)
- âœ… Basic RAG examples with ChromaDB and FAISS
- âœ… Hybrid retrieval prototype (TF-IDF + embeddings)
- âœ… CrossEncoderReranker implementation (LLM-based scoring)
- âœ… Module-specific project files for all 14 modules
- âš ï¸ **848 TODOs** across all 14 exercise files
- âš ï¸ **18 Python files** using direct `OpenAI()` instantiation

### Missing Infrastructure âŒ

- âŒ `llm/` directory (Responses API wrapper)
- âŒ `rag/` directory (rerankers, retrievers, eval)
- âŒ `tests/` directory (pytest autograders)
- âŒ `agents/` directory (tool guardrails, policy)
- âŒ `metrics/` directory (observability, tracing)
- âŒ `templates/` directory (FastAPI skeleton)
- âŒ `capstone/` project directory
- âŒ `config/` directory (model configurations)
- âŒ `data/` directory (evaluation datasets)
- âŒ `snippets/` directory (reusable prompts)
- âŒ `runs/` directory (run tracking)
- âŒ CI/CD infrastructure (Dockerfile, GitHub Actions, Makefile)
- âŒ Documentation (safety.md, role-guidelines.md, CONTRIBUTING.md)

---

## Implementation Plan by Commit

### COMMIT 1: Update README Status Claim

**Type**: Documentation
**Effort**: 15 minutes
**Risk**: Low

#### Files Changed
- `README.md`

#### Changes
```diff
- ## âœ… Status: 100% Complete - Production Ready
+ ## ðŸš§ Status: Beta - Actively Maintained
+
+ **Known Gaps** (tracked in CHANGELOG.md):
+ - 848 exercise TODOs being completed progressively
+ - RAG reranker migrating from LLM scoring to HuggingFace cross-encoder
+ - CI/CD examples being enhanced for production readiness
```

#### Testing
- [ ] Preview README in GitHub to verify markdown rendering
- [ ] Ensure status badge updates correctly

#### Dependencies
None

---

### COMMIT 2: Create LLM Client with Responses API

**Type**: Core infrastructure
**Effort**: 2 hours
**Risk**: Medium (foundation for many other commits)

#### Files Created
- `llm/__init__.py`
- `llm/client.py`

#### New File: `llm/__init__.py`
```python
"""LLM client abstraction layer for OpenAI Responses API."""

from .client import LLMClient

__all__ = ["LLMClient"]
```

#### New File: `llm/client.py`
```python
"""
LLM Client wrapper using OpenAI Responses API.

This module provides a standardized interface for LLM interactions
using the modern Responses API pattern.
"""

from typing import Dict, List, Any, Optional
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class LLMClient:
    """Unified client for LLM interactions using Responses API."""

    def __init__(self, model: str = None):
        """
        Initialize LLM client.

        Args:
            model: Model name (defaults to OPENAI_MODEL env var or gpt-5)
        """
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-5")

    def generate(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> Any:
        """
        Generate completion using Responses API.

        Args:
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature (0-2)
            max_tokens: Maximum tokens in response
            **kwargs: Additional API parameters

        Returns:
            OpenAI response object
        """
        return self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )

    def structured(
        self,
        messages: List[Dict[str, str]],
        schema: dict,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> Any:
        """
        Generate structured output with strict JSON schema validation.

        Args:
            messages: List of message dicts
            schema: JSON schema dict with 'name', 'strict', and 'schema' keys
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            **kwargs: Additional API parameters

        Returns:
            OpenAI response object with validated JSON
        """
        return self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format={
                "type": "json_schema",
                "json_schema": schema,
                "strict": True
            },
            **kwargs
        )
```

#### Testing
```python
# Test script: test_llm_client.py
from llm.client import LLMClient

def test_basic_generation():
    client = LLMClient()
    response = client.generate([
        {"role": "user", "content": "Say hello in 5 words"}
    ])
    assert response.choices[0].message.content
    print("âœ“ Basic generation works")

def test_structured_output():
    client = LLMClient()
    schema = {
        "name": "greeting",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "message": {"type": "string"},
                "language": {"type": "string"}
            },
            "required": ["message", "language"],
            "additionalProperties": False
        }
    }
    response = client.structured(
        [{"role": "user", "content": "Greet me in JSON"}],
        schema=schema
    )
    assert response.choices[0].message.content
    print("âœ“ Structured output works")

if __name__ == "__main__":
    test_basic_generation()
    test_structured_output()
```

#### Dependencies
None

---

### COMMIT 3: Add Model Config System

**Type**: Configuration
**Effort**: 1 hour
**Risk**: Low

#### Files Created
- `config/models.example.yml`

#### Files Modified
- `shared/utils.py` (update `estimate_cost` function)

#### New File: `config/models.example.yml`
```yaml
# Example model configuration
# Copy to config/models.yml and customize
# Prices are illustrative - always check provider documentation

openai:
  gpt-5:
    input_per_1m: 5.00
    output_per_1m: 15.00
    context_window: 128000

  gpt-5-mini:
    input_per_1m: 0.30
    output_per_1m: 1.20
    context_window: 128000

  gpt-5-nano:
    input_per_1m: 0.10
    output_per_1m: 0.40
    context_window: 16000

  gpt-5-codex:
    input_per_1m: 6.00
    output_per_1m: 18.00
    context_window: 128000

anthropic:
  claude-sonnet-4-5:
    input_per_1m: 3.00
    output_per_1m: 15.00
    context_window: 200000

  claude-opus-4-1:
    input_per_1m: 15.00
    output_per_1m: 75.00
    context_window: 200000

  claude-haiku-3-5:
    input_per_1m: 0.80
    output_per_1m: 4.00
    context_window: 200000
```

#### Changes to `shared/utils.py`
```python
import yaml
import os

def load_model_config(config_path: str = "config/models.yml") -> dict:
    """Load model configuration from YAML file."""
    if os.path.exists(config_path):
        with open(config_path) as f:
            return yaml.safe_load(f)
    return {}

def estimate_cost(
    input_tokens: int,
    output_tokens: int,
    model: str,
    config: dict = None
) -> Dict[str, float]:
    """
    Estimate cost for a completion.

    Reads from config file or environment variables.
    If neither provided, prints "illustrative only" warning.

    Args:
        input_tokens: Number of input tokens
        output_tokens: Number of output tokens
        model: Model name
        config: Optional config dict (loaded from file if None)

    Returns:
        Dictionary with cost breakdown
    """
    # Try to load config
    if config is None:
        config = load_model_config()

    # Try environment variables
    cost_per_1k_input = os.getenv("COST_PER_1K_INPUT")
    cost_per_1k_output = os.getenv("COST_PER_1K_OUTPUT")

    if not config and not (cost_per_1k_input and cost_per_1k_output):
        print("âš ï¸  Costs are illustrative only. Set COST_PER_1K_INPUT/OUTPUT env vars or create config/models.yml")
        # Use default pricing as fallback
        pricing = {
            "gpt-5": {"input": 5.00, "output": 15.00},
            "gpt-5-mini": {"input": 0.30, "output": 1.20},
            # ... rest of defaults
        }
    elif cost_per_1k_input and cost_per_1k_output:
        # Use env vars
        input_cost = (input_tokens / 1_000_000) * float(cost_per_1k_input)
        output_cost = (output_tokens / 1_000_000) * float(cost_per_1k_output)
    else:
        # Use config file
        provider, model_name = model.split("/") if "/" in model else ("openai", model)
        if provider in config and model_name in config[provider]:
            model_config = config[provider][model_name]
            input_cost = (input_tokens / 1_000_000) * model_config["input_per_1m"]
            output_cost = (output_tokens / 1_000_000) * model_config["output_per_1m"]
        else:
            return {"error": f"Model {model} not found in config"}

    return {
        "input_cost": round(input_cost, 4),
        "output_cost": round(output_cost, 4),
        "total_cost": round(input_cost + output_cost, 4),
        "input_tokens": input_tokens,
        "output_tokens": output_tokens
    }
```

#### Testing
- [ ] Run cost optimization examples without env vars or config (verify warning prints)
- [ ] Run with `COST_PER_1K_INPUT=5.00 COST_PER_1K_OUTPUT=15.00` (verify uses env)
- [ ] Create `config/models.yml` and run (verify uses config)

#### Dependencies
Requires `pyyaml` (already in requirements.txt)

---

### COMMIT 4: Robust Tokenizer Fallback

**Type**: Bug fix
**Effort**: 30 minutes
**Risk**: Low

#### Files Modified
- `shared/utils.py`

#### Changes
```python
def get_encoding(model: str):
    """
    Get tiktoken encoding with fallback for unknown models.

    Args:
        model: Model name

    Returns:
        tiktoken Encoding object
    """
    import tiktoken

    try:
        return tiktoken.encoding_for_model(model)
    except KeyError:
        # Try o200k_base for newer models
        try:
            return tiktoken.get_encoding("o200k_base")
        except Exception:
            # Fallback to cl100k_base
            return tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str, model: str = "gpt-5") -> int:
    """
    Count tokens in text using appropriate encoding.

    Args:
        text: Text to tokenize
        model: Model name (for encoding selection)

    Returns:
        Token count
    """
    encoding = get_encoding(model)
    return len(encoding.encode(text))
```

#### Testing
```python
# Test with known and unknown models
from shared.utils import count_tokens

# Should work
assert count_tokens("Hello world", "gpt-5") > 0
assert count_tokens("Hello world", "gpt-4") > 0

# Should not crash on unknown model
assert count_tokens("Hello world", "bogus-model-xyz") > 0
print("âœ“ Tokenizer fallback works")
```

#### Dependencies
None (tiktoken already in requirements.txt)

---

### COMMIT 5: Structured Output Utilities

**Type**: Core utility
**Effort**: 2 hours
**Risk**: Medium

#### Files Created
- `shared/structured.py`

#### New File: `shared/structured.py`
```python
"""
Structured output utilities for reliable JSON generation.

This module provides helpers for generating and validating
structured outputs from LLMs using JSON schemas.
"""

from typing import Dict, Any, List
import json
import jsonschema
from llm.client import LLMClient


def ask_json(
    messages: List[Dict[str, str]],
    schema: dict,
    model: str = "gpt-5",
    temperature: float = 0.7,
    validate: bool = True
) -> dict:
    """
    Request structured JSON output with schema validation.

    Args:
        messages: List of message dicts
        schema: JSON schema dict
        model: Model to use
        temperature: Sampling temperature
        validate: Whether to validate output against schema

    Returns:
        Parsed and validated JSON dict

    Raises:
        json.JSONDecodeError: If response is not valid JSON
        jsonschema.ValidationError: If JSON doesn't match schema
    """
    client = LLMClient(model=model)
    response = client.structured(messages, schema=schema, temperature=temperature)

    # Parse JSON from response
    content = response.choices[0].message.content
    result = json.loads(content)

    # Validate against schema if requested
    if validate:
        jsonschema.validate(instance=result, schema=schema["schema"])

    return result


def create_json_schema(
    name: str,
    properties: Dict[str, dict],
    required: List[str] = None,
    additional_properties: bool = False
) -> dict:
    """
    Helper to create JSON schema for structured outputs.

    Args:
        name: Schema name
        properties: Property definitions
        required: List of required property names
        additional_properties: Allow additional properties

    Returns:
        JSON schema dict compatible with OpenAI structured outputs

    Example:
        >>> schema = create_json_schema(
        ...     name="user_info",
        ...     properties={
        ...         "name": {"type": "string"},
        ...         "age": {"type": "integer", "minimum": 0}
        ...     },
        ...     required=["name"]
        ... )
    """
    return {
        "name": name,
        "strict": True,
        "schema": {
            "type": "object",
            "properties": properties,
            "required": required or [],
            "additionalProperties": additional_properties
        }
    }


def extract_json_from_text(text: str) -> dict:
    """
    Extract JSON from text that may contain additional content.

    Args:
        text: Text potentially containing JSON

    Returns:
        Parsed JSON dict

    Raises:
        ValueError: If no valid JSON found
    """
    import re

    # Try to find JSON object in text
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if not json_match:
        raise ValueError("No JSON object found in text")

    return json.loads(json_match.group())


# Example usage
if __name__ == "__main__":
    # Define schema
    schema = create_json_schema(
        name="sentiment_analysis",
        properties={
            "sentiment": {
                "type": "string",
                "enum": ["positive", "negative", "neutral"]
            },
            "confidence": {
                "type": "number",
                "minimum": 0,
                "maximum": 1
            },
            "reasoning": {"type": "string"}
        },
        required=["sentiment", "confidence"]
    )

    # Request structured output
    messages = [{
        "role": "user",
        "content": "Analyze sentiment: 'I love this product!'"
    }]

    result = ask_json(messages, schema)
    print(f"Sentiment: {result['sentiment']}")
    print(f"Confidence: {result['confidence']}")
```

#### Testing
```python
from shared.structured import ask_json, create_json_schema

# Test schema creation
schema = create_json_schema(
    name="test",
    properties={"message": {"type": "string"}},
    required=["message"]
)
assert schema["name"] == "test"
assert schema["strict"] == True

# Test JSON extraction
result = ask_json(
    [{"role": "user", "content": "Say hello in JSON"}],
    schema
)
assert "message" in result
print("âœ“ Structured output utilities work")
```

#### Dependencies
- Requires `llm.client` (Commit 2)
- Uses `jsonschema` (already in requirements.txt)

---

### COMMIT 6: Cross-Encoder Reranker (HuggingFace)

**Type**: RAG enhancement
**Effort**: 3 hours
**Risk**: Medium (large model download)

#### Files Created
- `rag/__init__.py`
- `rag/rerankers.py`

#### Files Modified
- `requirements.txt` (add sentence-transformers)
- `11-advanced-rag/examples/advanced_reranking.py` (refactor to use new reranker)

#### Update: `requirements.txt`
```diff
+ # For advanced reranking
+ sentence-transformers>=2.2.0
```

#### New File: `rag/__init__.py`
```python
"""RAG (Retrieval-Augmented Generation) utilities."""

from .rerankers import CrossEncoderReranker, LLMReranker

__all__ = ["CrossEncoderReranker", "LLMReranker"]
```

#### New File: `rag/rerankers.py`
```python
"""
Reranking utilities for RAG pipelines.

Provides both cross-encoder (HuggingFace) and LLM-based reranking.
"""

from typing import List, Tuple, Optional
from dataclasses import dataclass
import numpy as np


@dataclass
class RankedDocument:
    """Document with ranking information."""
    content: str
    score: float
    original_rank: int
    metadata: dict = None


class CrossEncoderReranker:
    """
    Rerank documents using HuggingFace cross-encoder models.

    More accurate than bi-encoder (embedding similarity) but slower.
    Best used as a second-stage reranker on top-k results.
    """

    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """
        Initialize cross-encoder reranker.

        Args:
            model_name: HuggingFace model name
                Popular options:
                - cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, good quality)
                - cross-encoder/ms-marco-MiniLM-L-12-v2 (slower, better quality)
        """
        from sentence_transformers import CrossEncoder
        self.model = CrossEncoder(model_name)
        self.model_name = model_name

    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: int = 10
    ) -> List[RankedDocument]:
        """
        Rerank documents by relevance to query.

        Args:
            query: Search query
            documents: List of document texts
            top_k: Number of top documents to return

        Returns:
            List of RankedDocument objects sorted by score
        """
        if not documents:
            return []

        # Create query-document pairs
        pairs = [[query, doc] for doc in documents]

        # Score pairs
        scores = self.model.predict(pairs)

        # Create ranked documents
        ranked = [
            RankedDocument(
                content=doc,
                score=float(score),
                original_rank=i
            )
            for i, (doc, score) in enumerate(zip(documents, scores))
        ]

        # Sort by score descending
        ranked.sort(key=lambda x: x.score, reverse=True)

        return ranked[:top_k]

    def score_pair(self, query: str, document: str) -> float:
        """Score a single query-document pair."""
        return float(self.model.predict([[query, document]])[0])


class LLMReranker:
    """
    Rerank documents using LLM scoring (legacy method).

    Less accurate and more expensive than cross-encoder, but can
    provide reasoning. Kept for comparison and educational purposes.
    """

    def __init__(self, model: str = "gpt-5"):
        """Initialize LLM reranker."""
        from llm.client import LLMClient
        self.client = LLMClient(model=model)
        self.model = model

    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: int = 10
    ) -> List[RankedDocument]:
        """
        Rerank documents using LLM to score relevance.

        Args:
            query: Search query
            documents: List of document texts
            top_k: Number of top documents to return

        Returns:
            List of RankedDocument objects sorted by score
        """
        if not documents:
            return []

        ranked = []

        for i, doc in enumerate(documents):
            score = self._score_document(query, doc)
            ranked.append(RankedDocument(
                content=doc,
                score=score,
                original_rank=i
            ))

        ranked.sort(key=lambda x: x.score, reverse=True)
        return ranked[:top_k]

    def _score_document(self, query: str, document: str) -> float:
        """Score a document's relevance to query (0-10 scale)."""
        prompt = f"""Rate how relevant this document is to the query on a scale of 0-10.
Only respond with the number.

Query: {query}

Document: {document}

Relevance score (0-10):"""

        response = self.client.generate(
            [{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )

        try:
            score = float(response.choices[0].message.content.strip())
            return min(max(score, 0), 10) / 10  # Normalize to 0-1
        except ValueError:
            return 0.5  # Default if parsing fails


# Example usage
if __name__ == "__main__":
    # Sample documents
    docs = [
        "The cat sat on the mat.",
        "Python is a programming language.",
        "The dog played in the park.",
        "Machine learning uses algorithms to learn patterns."
    ]

    query = "Tell me about programming"

    # Cross-encoder reranking
    print("Cross-Encoder Reranking:")
    reranker = CrossEncoderReranker()
    results = reranker.rerank(query, docs, top_k=3)

    for i, doc in enumerate(results, 1):
        print(f"{i}. [{doc.score:.3f}] {doc.content}")
```

#### Refactor: `11-advanced-rag/examples/advanced_reranking.py`
```python
# Update imports at top of file
from rag.rerankers import CrossEncoderReranker, LLMReranker

# Update Example 1 to use new CrossEncoderReranker
# Keep LLM scoring as Example 2 labeled "Legacy Method"
```

#### Testing
```bash
# Install dependencies
pip install sentence-transformers

# Test script
python -c "
from rag.rerankers import CrossEncoderReranker
reranker = CrossEncoderReranker()
docs = ['Python is great', 'The sky is blue']
results = reranker.rerank('programming', docs)
print(f'âœ“ Reranker works: {results[0].content}')
"

# Run updated RAG example
python 11-advanced-rag/examples/advanced_reranking.py
```

#### Dependencies
- Requires `llm.client` (Commit 2) for LLMReranker
- Downloads ~500MB model on first run (ms-marco-MiniLM-L-6-v2)

---

### COMMIT 7: Pytest Autograders (Modules 01-03)

**Type**: Testing infrastructure
**Effort**: 4 hours
**Risk**: Low

#### Files Created
- `tests/__init__.py`
- `tests/conftest.py`
- `tests/test_mod01_fundamentals.py`
- `tests/test_mod02_zero_shot.py`
- `tests/test_mod03_few_shot.py`
- `pytest.ini`

#### New File: `pytest.ini`
```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --strict-markers
    --tb=short
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
```

#### New File: `tests/__init__.py`
```python
"""Test suite for prompt engineering curriculum."""
```

#### New File: `tests/conftest.py`
```python
"""Pytest configuration and shared fixtures."""

import pytest
import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))


@pytest.fixture
def mock_llm_response():
    """Mock LLM response for testing without API calls."""
    class MockResponse:
        def __init__(self, content):
            self.choices = [type('obj', (object,), {
                'message': type('obj', (object,), {'content': content})()
            })()]

    return MockResponse


@pytest.fixture
def sample_documents():
    """Sample documents for RAG testing."""
    return [
        "Python is a high-level programming language.",
        "Machine learning is a subset of artificial intelligence.",
        "The Eiffel Tower is located in Paris, France.",
        "Photosynthesis is the process by which plants make food."
    ]
```

#### New File: `tests/test_mod01_fundamentals.py`
```python
"""Tests for Module 01: Fundamentals."""

import pytest
from shared.utils import LLMClient, count_tokens, estimate_cost


class TestLLMClient:
    """Test LLM client initialization and basic operations."""

    def test_client_initialization_openai(self):
        """Test OpenAI client initialization."""
        client = LLMClient("openai")
        assert client.provider == "openai"
        assert client.default_model is not None

    def test_client_initialization_anthropic(self):
        """Test Anthropic client initialization."""
        client = LLMClient("anthropic")
        assert client.provider == "anthropic"
        assert client.default_model is not None

    @pytest.mark.skip(reason="Requires API key")
    def test_basic_completion(self):
        """Test basic completion (requires API key)."""
        client = LLMClient("openai")
        response = client.complete("Say hello")
        assert len(response) > 0


class TestTokenCounting:
    """Test token counting utilities."""

    def test_count_tokens_basic(self):
        """Test basic token counting."""
        tokens = count_tokens("Hello world")
        assert tokens > 0
        assert isinstance(tokens, int)

    def test_count_tokens_empty(self):
        """Test counting tokens in empty string."""
        tokens = count_tokens("")
        assert tokens == 0

    def test_count_tokens_long_text(self):
        """Test counting tokens in longer text."""
        text = "This is a longer piece of text " * 100
        tokens = count_tokens(text)
        assert tokens > 100


class TestCostEstimation:
    """Test cost estimation utilities."""

    def test_estimate_cost_basic(self):
        """Test basic cost estimation."""
        cost = estimate_cost(1000, 500, "gpt-5")
        assert "input_cost" in cost
        assert "output_cost" in cost
        assert "total_cost" in cost
        assert cost["total_cost"] > 0

    def test_estimate_cost_zero_tokens(self):
        """Test cost estimation with zero tokens."""
        cost = estimate_cost(0, 0, "gpt-5")
        assert cost["total_cost"] == 0


def test_module_imports():
    """Test that all fundamental module imports work."""
    # This ensures the module structure is correct
    from shared import utils, prompts
    assert utils is not None
    assert prompts is not None
```

#### New File: `tests/test_mod02_zero_shot.py`
```python
"""Tests for Module 02: Zero-Shot Prompting."""

import pytest
from shared.utils import LLMClient


class TestZeroShotPatterns:
    """Test zero-shot prompting patterns."""

    def test_instruction_clarity(self):
        """Test that clear instructions are more effective."""
        # This would compare vague vs clear prompts
        # Implementation would require actual API calls or mocking
        pass

    def test_context_provision(self):
        """Test that providing context improves results."""
        pass


class TestPromptTemplates:
    """Test zero-shot prompt templates."""

    def test_classification_template(self):
        """Test classification prompt template."""
        from shared.prompts import create_classification_prompt
        prompt = create_classification_prompt(
            text="This is great!",
            categories=["positive", "negative", "neutral"]
        )
        assert "positive" in prompt
        assert "negative" in prompt
```

#### New File: `tests/test_mod03_few_shot.py`
```python
"""Tests for Module 03: Few-Shot Learning."""

import pytest
from shared.utils import LLMClient


class TestFewShotExamples:
    """Test few-shot example selection and formatting."""

    def test_example_formatting(self):
        """Test that examples are formatted correctly."""
        examples = [
            {"input": "2+2", "output": "4"},
            {"input": "3+3", "output": "6"}
        ]
        # Test formatting logic
        pass

    def test_example_selection(self):
        """Test selecting relevant examples."""
        # Test semantic similarity-based selection
        pass


class TestFewShotPerformance:
    """Test that few-shot improves over zero-shot."""

    @pytest.mark.skip(reason="Requires API calls")
    def test_accuracy_improvement(self):
        """Test that few-shot improves accuracy."""
        # Compare zero-shot vs few-shot on test cases
        pass
```

#### Testing
```bash
# Install pytest
pip install pytest pytest-cov

# Run all tests
pytest -v

# Run with coverage
pytest --cov=shared --cov=llm --cov=rag

# Run only fast tests
pytest -m "not slow"
```

#### Dependencies
- Requires shared/utils.py, llm/client.py to exist

---

## Testing Strategy

### Unit Tests
- Test individual functions in isolation
- Mock external dependencies (API calls)
- Fast execution (< 1s per test)

### Integration Tests
- Test interactions between modules
- May use actual API calls (marked with `@pytest.mark.slow`)
- Require API keys in environment

### Test Organization
```
tests/
â”œâ”€â”€ test_mod01_fundamentals.py    # Shared utilities
â”œâ”€â”€ test_mod02_zero_shot.py        # Zero-shot prompting
â”œâ”€â”€ test_mod03_few_shot.py         # Few-shot learning
â”œâ”€â”€ test_mod04_chain_of_thought.py # COT reasoning
â”œâ”€â”€ ... (remaining modules)
â””â”€â”€ conftest.py                    # Shared fixtures
```

### Running Tests
```bash
# All tests
pytest

# Specific module
pytest tests/test_mod01_fundamentals.py

# Fast tests only
pytest -m "not slow"

# With coverage
pytest --cov=. --cov-report=html
```

---

## Phased Rollout Recommendation

### Phase 1: Foundation (Weeks 1-2) âœ… COMPLETED
**Commits**: 1-10
**Focus**: Core infrastructure and API modernization

- [x] COMMIT 1: README status update
- [x] COMMIT 2: LLM client with Responses API
- [x] COMMIT 3: Model config system
- [x] COMMIT 4: Tokenizer fallback
- [x] COMMIT 5: Structured output utilities
- [x] COMMIT 6: Cross-encoder reranker
- [x] COMMIT 7: Pytest autograders (Modules 01-03)
- [x] COMMIT 8: Deterministic mode utilities
- [x] COMMIT 9: Sampling parameter convention
- [x] COMMIT 10: Chain step utilities

**Deliverable**: âœ… Working LLM abstraction layer, basic testing infrastructure, improved RAG reranking

### Phase 2: RAG & Production (Weeks 3-4) âœ… COMPLETED
**Commits**: 11-19
**Focus**: Advanced RAG features and production tooling

- [x] COMMIT 11: Role guidelines documentation
- [x] COMMIT 12: Context router & summarizer
- [x] COMMIT 13: Hybrid retrieval with fusion
- [x] COMMIT 14: Query understanding pipeline
- [x] COMMIT 15: RAG evaluation harness
- [x] COMMIT 16: Observability & metrics
- [x] COMMIT 17: Safety documentation
- [x] COMMIT 18: Tool guardrails
- [x] COMMIT 19: FastAPI skeleton

**Deliverable**: âœ… Production-ready RAG system with evals, metrics, and safety

### Phase 3: Testing & Capstone (Weeks 5-6) ðŸš§ IN PROGRESS
**Commits**: 20-25
**Focus**: Complete testing coverage, CI/CD, and capstone project

- [x] COMMIT 20: Pytest + solutions policy âœ…
- [x] COMMIT 21: Run tracking system âœ…
- [x] COMMIT 22: Cost-aware labs âœ…
- [x] COMMIT 23: CI + Docker + Makefile âœ…
- [ ] COMMIT 24: Capstone project
- [ ] COMMIT 25: UX printing utilities

**Deliverable**: Fully tested curriculum with CI/CD and capstone project

---

## Risk Mitigation

### High-Risk Items

1. **18 files OpenAI SDK refactoring** (Commits 2, 9)
   - **Mitigation**: Create migration script to automate refactoring
   - **Testing**: Run all examples before/after to verify behavior unchanged
   - **Rollback**: Keep original files in `_backup/` until verified

2. **848 TODOs to complete** (Commit 20)
   - **Mitigation**: Focus on Modules 01-03 first, defer rest to Phase 3
   - **Testing**: Pytest autograders verify completions work
   - **Scope**: Can be completed incrementally over multiple PRs

3. **Responses API assumption** (Commit 2)
   - **Risk**: PR assumes OpenAI Responses API exists, may not
   - **Mitigation**: Verify OpenAI SDK docs before implementing
   - **Fallback**: Use standard chat.completions API with wrapper

### Medium-Risk Items

1. **Cross-encoder model size** (Commit 6)
   - **Impact**: ~500MB model download
   - **Mitigation**: Document in README, provide model selection guide
   - **Alternative**: Offer lighter model option

2. **CI/CD complexity** (Commit 23)
   - **Impact**: May confuse learners who just want to learn prompting
   - **Mitigation**: Make all CI/CD optional, document clearly
   - **Structure**: Keep in `templates/devops/` not root

### Low-Risk Items

- Documentation additions (Commits 1, 11, 17)
- Utility functions (Commits 4, 8, 21, 25)
- Config files (Commit 3)

---

## Success Criteria

### Must Have
- âœ… All 25 commits implemented
- âœ… Pytest suite passes with >80% coverage
- âœ… CI/CD pipeline green
- âœ… All examples run successfully
- âœ… Documentation complete and accurate

### Should Have
- âœ… Modules 01-03 exercises completed (100%)
- âœ… RAG evaluation showing measurable improvements
- âœ… Capstone project with working demo
- âš ï¸ Remaining modules exercises completed (50%+)

### Nice to Have
- â­ All 848 TODOs completed
- â­ Migration guide for external users
- â­ Video walkthrough of new features

---

## Next Steps

1. **Review this plan** with stakeholders
2. **Verify OpenAI Responses API** exists and works as described
3. **Create feature branch**: `git checkout -b feat/pr-10-02-25-course-hardening`
4. **Begin Phase 1 implementation**
5. **Regular check-ins** after each phase completion

---

## Appendix A: File Inventory

### New Directories (11)
```
llm/
rag/
config/
tests/
agents/
metrics/
templates/api_skeleton/
capstone/
snippets/
data/
runs/
```

### New Files (50+)

**Infrastructure**:
- llm/client.py, llm/__init__.py
- rag/rerankers.py, rag/retrievers.py, rag/eval.py, rag/__init__.py
- config/models.example.yml

**Utilities**:
- shared/structured.py
- shared/repro.py
- shared/chains.py
- shared/context.py
- shared/printing.py
- shared/runs.py

**Testing**:
- pytest.ini
- tests/__init__.py, tests/conftest.py
- tests/test_mod01.py through tests/test_mod14.py (14 files)

**Documentation**:
- docs/role-guidelines.md
- docs/safety.md
- CONTRIBUTING.md
- capstone/README.md
- capstone/RUBRIC.md

**Templates**:
- templates/api_skeleton/main.py
- templates/api_skeleton/routers/generate.py
- templates/api_skeleton/routers/rag.py

**DevOps**:
- Dockerfile
- docker-compose.yml
- Makefile
- .github/workflows/ci.yml

**Data**:
- data/rag_eval_min.jsonl
- snippets/house_system_prompt.md

### Modified Files (20+)

**Core**:
- README.md
- requirements.txt
- shared/utils.py

**RAG Examples**:
- 11-advanced-rag/examples/advanced_reranking.py
- 11-advanced-rag/examples/hybrid_retrieval.py
- 11-advanced-rag/examples/query_processing.py

**All Cost Examples** (Module 12):
- 12-prompt-optimization/examples/cost_optimization.py
- 12-prompt-optimization/examples/token_optimization.py
- 12-prompt-optimization/examples/performance_testing.py

**Direct OpenAI Usage** (18 files):
- All files identified in initial grep

---

## Appendix B: Dependencies

### Python Packages to Add
```
sentence-transformers>=2.2.0  # Commit 6
pytest>=7.4.0                 # Commit 7
pytest-cov>=4.1.0             # Commit 7
```

### Python Packages Already Present
```
openai>=1.12.0
anthropic>=0.18.0
langchain>=0.1.0
chromadb>=0.4.22
faiss-cpu>=1.7.4
pydantic>=2.5.0
jsonschema>=4.20.0
fastapi>=0.109.0
uvicorn>=0.27.0
pyyaml>=6.0.1
```

### External Services Required
- OpenAI API key (required)
- Anthropic API key (optional)

### Model Downloads
- Cross-encoder: ms-marco-MiniLM-L-6-v2 (~500MB, one-time)

---

**Document End**
