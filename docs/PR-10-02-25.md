# PR: Course hardening & productionization (API modernization, RAG robustness, evals, safety, CI/CD)

## âœ… Implementation Status

**Status**: âœ… COMPLETED (All 25 commits implemented)
**Implementation Date**: October 2025
**Detailed Plan**: See [PLAN-PR-10-02-25.md](./PLAN-PR-10-02-25.md) for complete implementation details

### Key Implementation Notes

This document represents the **original specification**. During implementation, some modifications were made and documented in PLAN-PR-10-02-25.md:

1. **Model Updates**: Updated from GPT-4.5 (speculative) to GPT-5 family (gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-codex) and Claude Sonnet 4.5 (September 2025 release)
2. **Dual Client Approach**: Retained `shared/utils.py` alongside new `llm/client.py` for backwards compatibility and educational value
3. **Educational Simplifications**: Capstone project uses simplified retrieval for clarity, with production extensions as optional exercises

All core objectives achieved. Refer to PLAN-PR-10-02-25.md for authoritative implementation details.

---

## Why

This PR turns the course into a production-credible, self-testing, and up-to-date learning resource. It modernizes API usage (OpenAI Responses + Structured Outputs), corrects model assumptions, upgrades RAG (hybrid + rerankers + evals), adds safety & observability, and completes exercise scaffolding.

## Scope (25 atomic commits)

Each numbered section below = one commit. Apply in order (top to bottom). Every commit is self-contained with context, change steps, and "how to test".

### 1) docs(readme): replace "production-ready" claim with honest status & gaps

**Context**  
README currently promises "100% completeâ€”production ready" while exercises have TODOs. That overpromises.

**Change**  
Edit README.md: replace the claim with "Beta (actively maintained)" and add a short "Known gaps" list.

**Patch sketch**
```diff
- âœ… Status: 100% Complete â€” Production Ready
+ ðŸš§ Status: Beta â€” Actively Maintained
+ Known gaps (tracked in /CHANGELOG.md):
+ - Some exercises still lack autograder assertions
+ - RAG reranker uses LLM scoring (will switch to cross-encoder)
+ - CI/CD example is minimal; see capstone
```

**Test**  
Preview README renders correctly.

### 2) feat(llm): introduce portable client wrapper; standardize on Responses API

**Context**  
Examples call chat.completions.create directly and hardcode model names. We want a consistent interface and modern API.

**Change**  
New: llm/client.py with generate() and structured() using OpenAI Responses API; optional Anthropic adapter later.

Replace direct calls in examples with this wrapper.

**Files**
- llm/client.py (new)
- Replace imports in modules that call OpenAI directly.

**Snippet**
```python
# llm/client.py
from pydantic import BaseModel
from openai import OpenAI

class LLMClient:
    def __init__(self, model: str):
        self.client = OpenAI()
        self.model = model

    def generate(self, messages, **kw):
        return self.client.responses.create(model=self.model, input=messages, **kw)

    def structured(self, messages, schema: dict, **kw):
        return self.client.responses.create(
            model=self.model,
            input=messages,
            response_format={"type": "json_schema", "json_schema": schema, "strict": True},
            **kw
        )
```

**Test**  
Run any module calling LLMClient().generate(...); verify response arrives.

### 3) chore(config): remove speculative pricing & model tables; replace with examples

**Context**  
Hardcoded prices and non-existent SKUs drift quickly.

**Change**  
New: config/models.example.yml with illustrative entries only.

Update utilities to read config (if present) but treat as non-authoritative.

**Files**
- config/models.example.yml (new)
- shared/utils.py (read config if provided; otherwise skip pricing math)

**Test**  
`python -c "import yaml; print('ok')"` ; run examples; no price assertions.

### 4) fix(tokens): robust tokenizer fallback for unknown models

**Context**  
Direct encoding_for_model("unknown") raises errors.

**Change**  
New helper get_encoding(model) â†’ try model mapping; fallback to o200k_base then cl100k_base.

**Snippet**
```python
def get_encoding(model: str):
    import tiktoken
    try:
        return tiktoken.encoding_for_model(model)
    except Exception:
        try:
            return tiktoken.get_encoding("o200k_base")
        except Exception:
            return tiktoken.get_encoding("cl100k_base")
```

**Test**  
Call with a bogus model; ensure no crash.

### 5) feat(structured): make strict JSON the default pattern

**Context**  
Brittle string parsing in many labs.

**Change**  
New: shared/structured.py with ask_json(messages, schema) using Responses API strict schemas.

Refactor examples in Modules 02/08/10 to use it.

**Snippet**
```python
def ask_json(client: LLMClient, messages, schema):
    r = client.structured(messages, schema=schema)
    return r.output[0].content[0].text  # or r.parsed if available in your SDK version
```

**Test**  
Run structured output labs; validate with jsonschema.

### 6) feat(rag): add CrossEncoder reranker; demote LLM "0â€“10" scoring

**Context**  
LLM-scored rerank is non-deterministic and costly.

**Change**  
New: rag/rerankers.py with CrossEncoderReranker (HuggingFace).

Update advanced RAG lesson to use it by default; keep LLM scoring as optional.

**Snippet**
```python
# rag/rerankers.py
from sentence_transformers import CrossEncoder

class CrossEncoderReranker:
    def __init__(self, model="cross-encoder/ms-marco-MiniLM-L6-v2"):
        self.model = CrossEncoder(model)

    def rerank(self, query: str, docs: list[str], top_k=10):
        pairs = [[query, d] for d in docs]
        scores = self.model.predict(pairs).tolist()
        ranked = sorted(zip(docs, scores), key=lambda x:x[1], reverse=True)
        return ranked[:top_k]
```

**Test**  
Run RAG notebook: confirm reranking changes order and improves hit@k on sample set.

### 7) test(exercises): complete print/test stubs; add pytest autograders (mods 01â€“03)

**Context**  
Learners need verification.

**Change**  
Add tests/test_mod01.py, test_mod02.py, test_mod03.py.

Replace "TODO: print" with assertions.

**Test**  
`pytest -q` passes.

### 8) feat(repro): deterministic mode utilities

**Context**  
Runs vary; hard to debug.

**Change**  
New: shared/repro.py with set_seed(), run metadata capture, and env logging.

Add --deterministic flag to example CLIs.

**Test**  
Run twice with --deterministic; outputs match.

### 9) style(sampling): pick one sampler knob; refactor calls

**Context**  
Using both temperature and top_p confuses learners.

**Change**  
Course-wide convention: default to temperature only (or top_p onlyâ€”pick one, stay consistent).

Update llm/client.py to accept only our chosen knob; error if both provided.

**Snippet**
```python
if "temperature" in kw and "top_p" in kw:
    raise ValueError("Use either temperature OR top_p, not both (course convention).")
```

**Test**  
Run examples; ensure no dual-knob usage.

### 10) feat(chaining): typed chain steps with logging & retries

**Context**  
Prompt chains are ad-hoc.

**Change**  
New: shared/chains.py with @chainstep decorator that:

- Logs input/output
- Validates against a schema
- Retries on parse failure

**Snippet**
```python
def chainstep(schema: dict|None=None):
    def deco(fn):
        def wrapped(*a, **kw):
            out = fn(*a, **kw)
            # validate out by schema if provided; log artifacts
            return out
        return wrapped
    return deco
```

**Test**  
Run Module 05 chaining lab; see step logs.

### 11) docs(style): role & persona guidelines + reusable header block

**Context**  
Inconsistent role scoping across lessons.

**Change**  
New doc: docs/role-guidelines.md (system vs developer vs user).

Add snippets/house_system_prompt.md; update examples to import and prepend.

**Test**  
Examples include the header; outputs stabilize.

### 12) feat(context): ContextRouter + Summarizer for scalable context windows

**Context**  
Naive concatenation wastes tokens.

**Change**  
New: shared/context.py with:

- ContextRouter (scores salience; selects snippets)
- Summarizer (structured memory for long threads)

**Test**  
Run context lab on a long transcript; token count drops; answers still correct.

### 13) feat(rag): hybrid retrieval with fusion (BM25 + dense + metadata)

**Context**  
Dense-only is brittle.

**Change**  
New: rag/retrievers.py with HybridRetriever that:

- Runs BM25 and vector search
- Applies weighted fusion or RRF
- Supports metadata filters

**Snippet**
```python
def fuse(bm25_hits, dense_hits, alpha=0.5):
    # normalize scores to [0,1], then weighted sum
    ...
```

**Test**  
A/B: dense-only vs hybrid; check nDCG@10 improvements on sample corpus.

### 14) feat(rag): query understanding pipeline lab

**Context**  
The "query pipeline" is skeletal.

**Change**  
New notebook: 10-rag/03_query_understanding.ipynb

Intent classification â†’ query rewrite â†’ decomposition.

Provide rubric & sample solutions.

**Test**  
Run notebook; see improved retrieval vs raw query.

### 15) feat(evals): RAG eval harness (faithfulness, answer relevance, hit rate)

**Context**  
Evaluation is mostly prompts.

**Change**  
New: rag/eval.py with minimal dataset loader and metrics:

- Faithfulness (LLM-judge or rule-based)
- Answer relevancy
- Retrieval hit rate / context recall

New data: data/rag_eval_min.jsonl

**Test**  
`python -m rag.eval --dataset data/rag_eval_min.jsonl` prints metrics and a small report.

### 16) feat(metrics): observability & cost tracking

**Context**  
No consistent metrics pipeline.

**Change**  
New: metrics/tracing.py with decorators to capture latency, token counts, cache hits, failures.

Add /metrics endpoint in API skeleton (see Commit 19).

**Test**  
Run examples; see per-call logs and aggregated counters.

### 17) feat(safety): practical safety snippet (injection hygiene, PII, secrets)

**Context**  
Production requires safety.

**Change**  
New doc: docs/safety.md (prompt injection patterns, allow-listed tools, argument validation, light PII scrubbing).

Add a small red-team exercise in 14-production/01_safety_exercise.md.

**Test**  
Follow exercise: attack â†’ mitigation; confirm blocked behaviors.

### 18) feat(agents): tool guardrails (schemas, argument validation, rate limits)

**Context**  
Agents misuse tools if unguarded.

**Change**  
- agents/tools.py: define tool schemas and validation
- agents/policy.py: allow-list + misuse checks
- Add "safe fallback" path on invalid args

**Snippet**
```python
def call_tool(tool, args):
    validate(args, tool.schema)
    try:
        return tool.fn(**args)
    except RateLimitError:
        return {"error": "rate_limited", "retry_in": 5}
```

**Test**  
Try invalid args â†’ see validation error; valid call â†’ success.

### 19) feat(api): FastAPI skeleton with health, metrics, and error handlers

**Context**  
Servers vary across examples.

**Change**  
New: templates/api_skeleton/:

- main.py (FastAPI), routers/generate.py, routers/rag.py
- CORS, /healthz, /metrics, error middleware

**Test**  
`uvicorn templates.api_skeleton.main:app --reload`; `curl /healthz` â†’ 200 OK.

### 20) test(autograder): pytest per module + solutions policy

**Context**  
Solutions are mixed/minified.

**Change**  
- tests/ folder per module with minimal assertions
- solutions/ gated by env var (e.g., ALLOW_SOLUTIONS=1)
- Document policy in CONTRIBUTING.md

**Test**  
`pytest -q` green; `ALLOW_SOLUTIONS=1` exposes solutions locally.

### 21) feat(runs): capture run params & prompt hash

**Context**  
We need apples-to-apples comparisons.

**Change**  
New: shared/runs.py with RunRecord(model, temperature, seed, prompt_hash, tokens_in/out, cost) and writer to runs/*.jsonl.

**Snippet**
```python
from dataclasses import dataclass, asdict
@dataclass
class RunRecord: ...
def log_run(rr: RunRecord, path="runs/history.jsonl"):
    with open(path, "a") as f: f.write(json.dumps(asdict(rr))+"\n")
```

**Test**  
Run any example; verify a new line in runs/history.jsonl.

### 22) feat(cost): cost-aware labs without hardcoded prices

**Context**  
We still want budgeting, not stale numbers.

**Change**  
New: read optional env COST_PER_1K_INPUT/COST_PER_1K_OUTPUT in examples

If not provided, print "illustrative only"

**Test**  
Set env; run optimization lab; see cost estimates.

### 23) ci(docker): minimal CI + Docker + Compose

**Context**  
Make deployability tangible.

**Change**  
- Dockerfile (multi-stage), compose.yaml
- .github/workflows/ci.yml: lint â†’ unit tests â†’ build image
- Makefile: make setup/test/build/run

**Test**
- `make test` locally
- CI passes on PR
- `docker compose up` runs API skeleton

### 24) feat(capstone): end-to-end RAG with evals & metrics

**Context**  
Learners need a portfolio artifact.

**Change**  
New project: capstone/ with requirements:

Document Q&A, Hybrid + reranker, evals (Commit 15), observability (Commit 16), API (Commit 19)

Rubric in capstone/RUBRIC.md

**Test**  
Follow capstone README; produce an eval report; screenshot metrics.

### 25) ux(print): tidy CLI output helpers

**Context**  
Ad-hoc prints are noisy.

**Change**  
New: shared/printing.py with print_answer(), print_sources(), print_metrics()

Replace scattered prints in examples/labs.

**Snippet**
```python
def print_answer(ans:str):
    print("\n=== ANSWER ===\n" + ans.strip())
```

**Test**  
Run any lab; see standardized output sections.

## Migration notes

- **Model names**: set via .env or config/models.yml; we no longer hardcode speculative SKUs.
- **API surface**: All new examples call llm/Client (Responses API).
- **Exercises**: Run pytest after each section to confirm progress.
- **Costs**: Provide env prices for budgeting exercises; otherwise, examples print "illustrative only".

## How to review

- Read commit titles; each has rationale + exact edits.
- You can cherry-pick any subset if you prefer staged rollout.
- The capstone depends on commits 6, 13, 15, 16, 19.

## How to test locally

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
make setup
pytest -q
uvicorn templates.api_skeleton.main:app --reload
```
